{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HF2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matech96/DeepLearningAVizualisInformatikaban/blob/master/HF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgexO14zLS-J",
        "colab_type": "text"
      },
      "source": [
        "#Deep Learning Alkalmazása a Vizuális Informatikában\n",
        "##2. Házi Feladat\n",
        "\n",
        "###1. Rész\n",
        "\n",
        "Valósíts meg egy paraméterezhető konvolúciós neurális hálózatot, amely osztályozásra képes. A hálózat paraméterei a következők:\n",
        "\n",
        "\n",
        "*   nC: Az osztályok száma\n",
        "*   nFeat: Az első réteg kimeneti csatornaszáma. Az ezt követő rétegek be- és kimeneti csatornaszáma egyezzen ezzel meg, majd minden leskálázó (strided konvolúciós) réteg duplázza ezt meg.\n",
        "*   nLevels: A háló szintjeinek száma. Egy szintnek az azonos térbeli kiterjedésű tenzorokon operáló rétegeket nevezzük (2 leskálázás közt). (Tipp: használj adaptív poolingot az osztályozó réteg előtt, hogy a változó szint szám ne okozzon problémát.)\n",
        "*   layersPerLevel: Az egy szinten található konvolúciós rétegek száma.\n",
        "*   kernelSize: A konvolúciós rétegek mérete.\n",
        "*   nLinType: Kategorikus változó, amellyel a nemlinearitás típusát állíthatja (hogy hány és milyen függvények közül lehet választani önre van bízva)\n",
        "*   bNorm: Bináris változó, amellyel állítható, hogy a konvolúciós rétegekbe teszünk-e BatchNormot.\n",
        "*   dropOut: Az osztályozó réteget közvetlenül megelőző dropout réteg bemeneti valószínűsége\n",
        "*   residual: Bináris változó, True érték esetén minden szinten valósíts meg egy reziduális kapcsolatot a szint bemenete és a szint végén megjelenő leskálázó réteg bemenete közt.\n",
        "\n",
        "Tipp: Érdemes ehhez írni először két külön modult, ami egy réteget (batchnormmal, dropouttal és nemlinearitással) valósít meg és egyet, ami meg ezekből egy szintet legózik össze. Ezekből aránylag könnyen összelegózható a háló.\n",
        "\n",
        "Tipp 2: A PyTorchnak van nn.ModuleList osztálya. Ez egy lista, amiben rétegek vannak, de ha sima lista változóba tesztek egyszerre több nn.Module-t, annak az optimizer nem fogja megkapni. Ez azért van, mert amikor egy nn.Module-tól leszármazó osztálytól elkéritek a .parameters()-t, akkor az végignézi az objektum összes tagváltozót, hogy van-e neki .parameters() függvénye (és ezt szépen rekurzívan végigcsinálja). A Listának pedig nincs, hiába vannak benne olyan elemek, amiknek van. +1: van nn.Sequential is, ami ugyanaz, csak a forward függvénye is felül van csapva, és szépen sorban meghívja a belül lévő rétegeket.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv_chP9pZP9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "10e93bb3-541a-46d0-be12-5fbaf520ceab"
      },
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install bayesian-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.45.0)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.18.2)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.2.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.27.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (46.1.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.6.0.post2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (0.34.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (2.21.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (3.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2019.11.28)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.18.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-0HhmjRLRGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch as th\n",
        "from torch import nn\n",
        "\n",
        "class Conv(nn.Sequential):\n",
        "  def __init__(self, in_ch, out_ch, kernel_size, act_name, is_bnorm, stride=1):\n",
        "    super().__init__()\n",
        "    act_fn = str_to_act_fn(act_name)\n",
        "    self.add_module(f\"conv\", nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=kernel_size, padding=kernel_size//2, stride=stride))\n",
        "    self.add_module(f\"{act_name}\", act_fn())\n",
        "    if is_bnorm:\n",
        "      self.add_module(f\"bnorm\", nn.BatchNorm2d(out_ch))\n",
        "\n",
        "class Level(nn.Sequential):\n",
        "  def __init__(self, ch, n_conv, kernel_size, act_name, is_bnorm):\n",
        "    super().__init__()\n",
        "    for conv_id in range(n_conv-1):\n",
        "      self.add_module(f\"conv_{conv_id}\", Conv(ch, ch, kernel_size, act_name, is_bnorm))\n",
        "    # self.add_module(f\"conv_{conv_id}\", Conv(in_ch, out_ch, kernel_size, act_name, is_bnorm, stride=2))\n",
        "\n",
        "def str_to_act_fn(act_name):\n",
        "  return nn.modules.activation.__dict__[act_name]\n",
        "def get_available_act_names():\n",
        "  return [name for name in nn.modules.activation.__dict__.keys() if name[0].isupper() and name != 'F']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wwh8fRVZGPWa",
        "colab": {}
      },
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self, n_classes, color_ch, img_side, first_out_ch, n_conv, kernel_size, act_name, is_bnorm, is_residual, n_levels):\n",
        "    super().__init__()\n",
        "    self.is_residual = is_residual\n",
        "    self.first = Conv(in_ch=color_ch, out_ch=first_out_ch, kernel_size=kernel_size, act_name=act_name, is_bnorm=False)\n",
        "    self.convs = nn.ModuleList([Level(ch=first_out_ch*(2**layer_id), \n",
        "                                      # out_ch=first_out_ch*(2**(layer_id+1)), \n",
        "                                      n_conv=n_conv, \n",
        "                                      kernel_size=kernel_size,\n",
        "                                      act_name=act_name,\n",
        "                                      is_bnorm=is_bnorm) for layer_id in range(n_levels)])\n",
        "    self.stride_convs = nn.ModuleList([Conv(in_ch=first_out_ch*(2**layer_id), \n",
        "                                            out_ch=first_out_ch*(2**(layer_id+1)), \n",
        "                                            kernel_size=kernel_size,\n",
        "                                            act_name=act_name,\n",
        "                                            is_bnorm=is_bnorm,\n",
        "                                            stride=2) for layer_id in range(n_levels)])\n",
        "    # self.pools = nn.ModuleList([nn.MaxPool2d(kernel_size=2) for _ in range(n_levels)])\n",
        "    lst_side = img_side // (2**n_levels)\n",
        "    lst_ch = first_out_ch * (2**(n_levels))\n",
        "    self.fc = nn.Linear(lst_side*lst_side*lst_ch, n_classes)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x_out = self.first(x)\n",
        "    for conv, stride_conv in zip(self.convs, self.stride_convs):\n",
        "    # for conv in self.convs:\n",
        "      x_in = x_out\n",
        "\n",
        "      x_out = conv(x_in)\n",
        "      if self.is_residual:\n",
        "        x_out = th.add(x_in, x_out)      \n",
        "      x_out = stride_conv(x_out)\n",
        "\n",
        "    x_out = th.flatten(x_out, start_dim=1)\n",
        "    return self.fc(x_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89d2gEfcgBGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchsummary import summary\n",
        "# summary(NN(n_classes=10, color_ch=3, img_side=32, first_out_ch=8, n_conv=2, kernel_size=3, act_name='ReLU', is_bnorm=True, is_residual=True, n_levels=3), (3,32,32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFVaU6FBTQ5W",
        "colab_type": "text"
      },
      "source": [
        "###2. Rész\n",
        "\n",
        "Valósíts meg egy paraméterezhető neurális háló tanító függvényt. \n",
        "\n",
        "A függvény bemenetként megkapja a fenti neurális háló megkostruálásához szükséges paramétereket, az összes random seedet 42 értékre állítja, majd végrehajtja a neurális háló tanítását a CIFAR10 adatbázison.\n",
        "\n",
        "Használj Adam optimizert és Cosine Annealing tanulási ráta ütemezőt.\n",
        "\n",
        "A tanítás során minden epoch után validálj, és jegyezd fel a legjobb validációs pontosságot, és a tanítás végén ezt add vissza.\n",
        "\n",
        "A függvénynek további bemeneti paraméterei:\n",
        "\n",
        "*   bSize: A bacth méret\n",
        "*   lr: a tanulási ráta\n",
        "*   lr_ratio: a tanulási ráta ütemező eta_min paramétere és a kezdeti tanulási ráta hányadosa\n",
        "*   numEpoch: az epochok száma\n",
        "*   decay: a weight_decay paraméter értéke\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsZhpkP1UaV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "\n",
        "class CoolCIFAR(pl.LightningModule):  \n",
        "  def __init__(self, first_out_ch, n_conv, kernel_size, act_name, is_bnorm, is_residual, n_levels, bSize, lr, lr_ratio, decay, numEpoch):\n",
        "    super().__init__()\n",
        "    self.bSize = bSize\n",
        "    self.lr = lr\n",
        "    self.lr_ratio = lr_ratio\n",
        "    self.decay = decay\n",
        "    self.numEpoch = numEpoch\n",
        "    self.model = NN(10, 3, 32, first_out_ch, n_conv, kernel_size, act_name, is_bnorm, is_residual, n_levels)\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    self.transform = transforms.Compose(\n",
        "                    [transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "  def parameters(self):\n",
        "    return self.model.parameters()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(CIFAR10('/cifar',download=True, train=True, transform=self.transform), batch_size=self.bSize)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(CIFAR10('/cifar',download=True, train=False, transform=self.transform), batch_size=self.bSize)\n",
        "\n",
        "  def test_dataloader(self):    \n",
        "    return self.val_dataloader()\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    opt = Adam(self.parameters(), lr=self.lr, weight_decay=self.decay)\n",
        "    sch = CosineAnnealingLR(opt, self.numEpoch, eta_min=self.lr_ratio)\n",
        "    return [opt], [sch]\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    res = self.forward(x)\n",
        "    loss = self.loss(res, y)\n",
        "    logs = {'train_loss': loss}\n",
        "    return {'loss': loss, 'log': logs}\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    return self.validation_step(batch, batch_idx)\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    res = self.forward(x)    \n",
        "    loss = self.loss(res, y)\n",
        "\n",
        "    y_ = th.argmax(res, dim=1)\n",
        "    acc = th.sum(y == y_).item() / (len(y) * 1.0)\n",
        "    return {'val_loss': loss, 'val_acc': acc}\n",
        "\n",
        "  def test_epoch_end(self, outputs):\n",
        "    self.test_res = self.validation_end(outputs)\n",
        "    return self.test_res\n",
        "\n",
        "  def validation_end(self, outputs):\n",
        "    avg_loss = th.stack([x['val_loss'] for x in outputs]).mean()\n",
        "    avg_acc = np.stack([x['val_acc'] for x in outputs]).mean()\n",
        "    # tensorboard_logs = {'val_loss': avg_loss}\n",
        "    res = {'avg_val_loss': avg_loss, 'avg_val_acc': avg_acc}\n",
        "    # print(res)\n",
        "    return res\n",
        "\n",
        "def train(first_out_ch, n_conv, kernel_size, act_name, is_bnorm, is_residual, n_levels, bSize, lr, lr_ratio, decay, numEpoch):\n",
        "  random.seed(42)\n",
        "  np.random.seed(42)\n",
        "  th.manual_seed(42)\n",
        "  th.backends.cudnn.deterministic = True\n",
        "  th.backends.cudnn.benchmark = False\n",
        "\n",
        "  model = CoolCIFAR(first_out_ch, n_conv, kernel_size, act_name, is_bnorm, is_residual, n_levels, bSize, lr, lr_ratio, decay, numEpoch)\n",
        "  # print(summary(model.model, (3, 32, 32)))\n",
        "  trainer = Trainer(min_epochs=1, max_epoch=numEpoch, show_progress_bar=False)#, train_percent_check=0.01, val_percent_check=0.01, fast_dev_run=True)\n",
        "  trainer.fit(model)\n",
        "  trainer.test()\n",
        "  return trainer.model.test_res['avg_val_acc']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-4XB6qjZLtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train(first_out_ch=8, n_conv=2, kernel_size=3, act_name='ReLU', is_bnorm=True, is_residual=True, n_levels=3, bSize=64, lr=0.1, lr_ratio=0.0, decay=0.01, numEpoch=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5qRygZGUata",
        "colab_type": "text"
      },
      "source": [
        "###3. Rész\n",
        "\n",
        "Valósíts meg hiperparaméter optimalizálást a Bayesian Optimization python könyvtár felhasználásával. A könyvtár itt érhető el: https://github.com/fmfn/BayesianOptimization\n",
        "\n",
        "A megoldás során a következőkre ügyelj:\n",
        "\n",
        "\n",
        "1.   A kezdeti random lépések száma legyen kb egyenlő a szabad paraméterek számának felével (5-6)\n",
        "2.   Az teljes lépésszám legyen ennek tízszerese (50-60)\n",
        "3.   Mivel a Bayesian Optimization függvény a bináris/diszkrét/integer paramétereket nem támogatja, ezért a folytonos értékek megfelelő konverziója az előző feladatrészben megvalósított függvény feladata.\n",
        "4.   Vannak persze olyan paraméterek, amiket nem kell optimalizálni (pl.: BatchNorm ami True, vagy az nClass ami 10 CIFAR10 esetében)\n",
        "5.   Az egyes paraméterek tartományát nektek kell ésszerűen meghatározni.\n",
        "6.   Az epochok közben menő progress barokat meg kiíratásokat érdemes eltüntetni, a Bayesian opt majd fog írogatni\n",
        "7.   Mivel ez sokáig tart érdemes ellenőrizni kevés adaton (van külön erre Sampler, amit a DataLoader-nek lehet átadni), hogy sikerül-e az overfitting.\n",
        "8.   Referenciaként, a CIFAR10-en olyan 91-2% pontosság az elfogadható, 95% számít jónak és 97%+ a state-of-the-art. Ezt persze a kis adatokon nem fogjátok elérni, mert az overfittinges.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOZCK8A7fK-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "import sys, os\n",
        "\n",
        "# Disable\n",
        "def blockPrint():\n",
        "    sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "# Restore\n",
        "def enablePrint():\n",
        "    sys.stdout = sys.__stdout__\n",
        "\n",
        "def f(first_out_ch, n_conv, kernel_size, n_levels, lr, lr_ratio, decay):\n",
        "  blockPrint()\n",
        "  val_acc = train(first_out_ch=int(first_out_ch),\n",
        "               n_conv=int(n_conv),\n",
        "               kernel_size=int(kernel_size)*2 + 1,\n",
        "               act_name='ReLU',\n",
        "               is_bnorm=True,\n",
        "               is_residual=True, \n",
        "               n_levels=int(n_levels),\n",
        "               bSize=64, \n",
        "               lr=lr, \n",
        "               lr_ratio=lr_ratio,\n",
        "               decay=decay,\n",
        "               numEpoch=50)\n",
        "  enablePrint()\n",
        "  return val_acc\n",
        "\n",
        "pbounds = {'first_out_ch': (2, 8),\n",
        "           'n_conv': (1, 3),\n",
        "           'kernel_size': (0, 2),\n",
        "           'n_levels': (1, 3),\n",
        "           'lr': (0.0001, 0.1),\n",
        "           'lr_ratio': (0.0001, 0.1),\n",
        "           'decay': (0.0001, 0.1)\n",
        "           }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=f,\n",
        "    pbounds=pbounds,\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "    init_points=5,\n",
        "    n_iter=50,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Pxg1TpjSmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}